# To read data in array from url
import numpy as np
import scipy as sp
from scipy import stats
from StringIO import StringIO
import urllib
url = "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data"
f = urllib.urlopen(url)
data = np.genfromtxt(StringIO(f.read()), delimiter=",")

data.shape # to know the dimensions mxn

# normalizing based on z-score
data[:,0:(data.shape[1]-1)] = stats.zscore(data[:,0:(data.shape[1]-1)])

# randomly shuffle data
np.random.shuffle(data)

# total samples
N = 4601

# split data as 60%, 20%, 20% split
training = data[:int(N*0.6)]
cv = data[int(N*0.6):int(N*0.8)]
test = data[int(N*0.8):]

# Logistic Regression - fit a model
import scikits as sklearn
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(C=0.1, penalty='l1')
model = lr.fit(training[:,0:-1], training[:,-1])

# predict cv set
cv_predict = model.predict(cv[:,0:-1])

# predict test set
cv_test = model.predict(test[:,0:-1])

# classification report
from sklearn.metrics import classification_report
print classification_report(cv[:,-1], cv_predict)
print classification_report(test[:,-1], cv_test)	

# SVM - fit a model
from sklearn.svm import SVC
for i in numpy.arange(0.1,10.0, 0.2):
    clf = SVC(C=i, kernel='linear')
    clf.fit(training[:,0:-1], training[:,-1])
    cv_predict = clf.predict(cv[:,0:-1])
    print i
    print classification_report(cv[:,-1], cv_predict)
    print '------------------------------------------'
# best recall for 0 at C=1.3 - 0.96


# -- Diagnostics and plots purposes -- #

# finding mean
for i in range(57):
    print "%s,%s"%(i, sp.mean(data[:, i]))

# finding median
for i in range(57):
    print "%s,%s"%(i, sp.median(data[:, i]))

# finding std
for i in range(57):
    print "%s,%s"%(i, sp.std(data[:, i]))

# plot
mean_data = np.loadtxt("/Users/Harit/Documents/personal/projects/python/machine-learning/mean", delimiter=',')
plot(mean_data)